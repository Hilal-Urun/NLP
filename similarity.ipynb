{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "U9B51zrVuXqB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install en_core_web_sm\n",
        "!pip install spacy nltk scikit-learn\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "bbQqAwSjveXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc1 = nlp(\"corona\")\n",
        "doc2=nlp(\"I have headache and something like flu but it can be also related to virus\")\n",
        "doc3=nlp(\"I went to doctor for pcr test, i hope i dont have any covid\")"
      ],
      "metadata": {
        "id": "1-7gZ6h6uXsS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc1.similarity(doc2))\n",
        "print(doc1.similarity(doc3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INLajE2QuXup",
        "outputId": "ee9e9686-e44e-45a9-ad2b-8fdab031b6f7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03827971584133683\n",
            "0.21223290349249319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''remove punctuation, lowercase, stem'''\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "\n",
        "def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wg5MoLKuXxF",
        "outputId": "403f2f45-3165-4d45-cdca-16af7dc7b2c9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim(\"corona\",\"I have headache and something like flu but it can be also related to virus\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZxTwmG_wao3",
        "outputId": "40fa7883-075a-4b96-ce12-61f353ce1d2a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim(\"corona\",\"I went to doctor for pcr test, i hope i dont have any covid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qOV6zdSzAc6",
        "outputId": "38d68c32-bc47-4768-a37f-06b73c336f5e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[\"corona\",\"I have headache and something like flu but it can be also related to virus\",\"I went to doctor for pcr test, i hope i dont have any covid\"]\n",
        "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
        "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
        "pairwise_similarity = tfidf * tfidf.T "
      ],
      "metadata": {
        "id": "zlEl6NNTuXzl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_similarity.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjFsY6PFuX1b",
        "outputId": "c2f6cf39-9c52-4b66-b9ff-cff5e2391d2e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer,util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "paragraph_emb1 = model.encode(\"Nanoparticle (NP)–protein complexes exhibit the “correct identity” of NP in biological media. Therefore, protein–NP interactions should be closely explored to understand and modulate the nature of NPs in medical implementations. This review focuses mainly on the physicochemical parameters such as dimension, surface chemistry, morphology of NPs, and influence of pH on the formation of protein corona and conformational changes of adsorbed proteins by different kinds of techniques. Also, the impact of protein corona on the colloidal stability of NPs is discussed. Uncontrolled protein attachment on NPs may bring unwanted impacts such as protein denaturation and aggregation. In contrast, controlled protein adsorption by optimal concentration, size, pH, and surface modification of NPs may result in potential implementation of NPs as therapeutic agents especially for disaggregation of amyloid fibrils. Also, the effect of NPs-protein corona on reducing the cytotoxicity and clinical implications such as drug delivery, cancer therapy, imaging and diagnosis will be discussed. Validated correlative physicochemical parameters for NP–protein corona formation frequently derived from protein corona fingerprints of NPs which are more valid than the parameters obtained only on the base of NP features. This review may provide useful information regarding the potency as well as the adverse effects of NPs to predict their behavior in vivo.\")\n",
        "query_emb = model.encode(\"corona\")\n",
        "sim_score = util.pytorch_cos_sim(query_emb, paragraph_emb1)\n",
        "print(sim_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "got8VWQKzj-n",
        "outputId": "90909889-088a-41d2-b475-4a22e72ecea8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2003]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_emb = model.encode(\"Nanoparticles (NPs) in contact with a biological medium are rapidly comprehended by a number of protein molecules resulting in the formation of an NP–protein complex called protein corona (PC). The cell sees the protein-coated NPs as the synthetic identity is masked by protein surfacing. The PC formation ultimately has a substantial impact on various biological processes including drug release, drug targeting, cell recognition, biodistribution, cellular uptake, and therapeutic efficacy. Further, the composition of PC is largely influenced by the physico-chemical properties of NPs viz. the size, shape, surface charge, and surface chemistry in the biological milieu. However, the change in the biological responses of the new substrate depends on the quantity of protein access by the NPs. The PC-layered NPs act as new biological entities and are recognized as different targeting agents for the receptor-mediated ingress of therapeutics in the biological cells. The corona-enveloped NPs have both pros and cons in the biological system. The review provides a brief insight into the impact of biomolecules on nanomaterials carrying cargos and their ultimate fate in the biological milieu.\")\n",
        "query_emb = model.encode(\"corona\")\n",
        "sim_score = util.pytorch_cos_sim(query_emb, paragraph_emb)\n",
        "print(sim_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGVu4sqo0BPt",
        "outputId": "8560d3d6-a792-4177-c448-370071bf5ff7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2250]])\n"
          ]
        }
      ]
    }
  ]
}